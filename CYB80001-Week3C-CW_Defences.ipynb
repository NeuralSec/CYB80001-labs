{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CYB80001 System Security Project\n",
    "Prepared by **Derui (Derek) Wang**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3C - Carlini&Wagner Attack and PSO Attack\n",
    "\n",
    "In this session, we will first implement the **Carlini & Wagner(C&W) attack** based on **Tensorflow** and **Keras**. We will then apply the attacks to a Keras classifier to evaluate the attacks.\n",
    "\n",
    "Second, we will implement PSO Attack which do not require gradients in the search of adversarial examples.\n",
    "\n",
    "Finally, we implement adversarial training as a defence on the Keras classifier. We will test the effectiveness of the defence against C&W attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "\n",
    "### Part 1 Implementing the C&W attack\n",
    "\n",
    "1.1 [Implementing C&W](#1_1)\n",
    "\n",
    "1.2 [Testing C&W](#1_2)\n",
    "\n",
    "\n",
    "### Part 2 Gradient-free Particle Swarm Optimisation attack\n",
    "\n",
    "2.1 [Implementing the PSO attack](#2_1)\n",
    "\n",
    "2.2 [Testing the attack](#2_2)\n",
    "\n",
    "\n",
    "### Part 3 Adversarial Training\n",
    "\n",
    "3.1 [Implementing adversarial training](#3_1)\n",
    "\n",
    "3.2 [Testing the defence](#3_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1. Implementing the C&W attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"1_1\"></a>\n",
    "\n",
    "## 1.1  Implementing C&W\n",
    "\n",
    "We will implement the **C&W attack** in this part. For reusing the C&W code in the following parts, we will implement it as a Python **class** object. \n",
    "\n",
    "We first import required packages and define hyper-parameters of the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "BINARY_SEARCH_STEPS = 9  # number of times to adjust the constant with binary search\n",
    "MAX_ITERATIONS = 10000   # number of iterations to perform gradient descent\n",
    "ABORT_EARLY = True       # if we stop improving, abort gradient descent early\n",
    "LEARNING_RATE = 1e-2    # larger values converge faster to less accurate results\n",
    "TARGETED = False          # should we target one specific class? or just be wrong?\n",
    "CONFIDENCE = 0           # how strong the adversarial example should be\n",
    "INITIAL_CONST = 1e-3     # the initial constant c to pick as a first guess\n",
    "\n",
    "BATCH_SIZE = 1    # the number of adversarial examples generated in parallel\n",
    "CLASS_NUM = 10    # class number of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement the C&W attack as a Python class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CW_Attack:\n",
    "    def __init__(self, sess, model, example_size, num_labels = CLASS_NUM, batch_size=BATCH_SIZE, confidence = CONFIDENCE, \n",
    "                 targeted = TARGETED, learning_rate = LEARNING_RATE, binary_search_steps = BINARY_SEARCH_STEPS, \n",
    "                 max_iterations = MAX_ITERATIONS, abort_early = ABORT_EARLY, initial_const = INITIAL_CONST, boxmin = 0, boxmax = 1):\n",
    "        \"\"\"\n",
    "        The L_2 optimized attack. \n",
    "\n",
    "        This attack is the most efficient and should be used as the primary \n",
    "        attack to evaluate potential defenses.\n",
    "\n",
    "        Returns adversarial examples for the supplied model.\n",
    "\n",
    "        confidence: Confidence of adversarial examples: higher produces examples\n",
    "          that are farther away, but more strongly classified as adversarial.\n",
    "        batch_size: Number of attacks to run simultaneously.\n",
    "        targeted: True if we should perform a targetted attack, False otherwise.\n",
    "        learning_rate: The learning rate for the attack algorithm. Smaller values\n",
    "          produce better results but are slower to converge.\n",
    "        binary_search_steps: The number of times we perform binary search to\n",
    "          find the optimal tradeoff-constant between distance and confidence. \n",
    "        max_iterations: The maximum number of iterations. Larger values are more\n",
    "          accurate; setting too small will require a large learning rate and will\n",
    "          produce poor results.\n",
    "        abort_early: If true, allows early aborts if gradient descent gets stuck.\n",
    "        initial_const: The initial tradeoff-constant to use to tune the relative\n",
    "          importance of distance and confidence. If binary_search_steps is large,\n",
    "          the initial constant is not important.\n",
    "        boxmin: Minimum pixel value (default -0.5).\n",
    "        boxmax: Maximum pixel value (default 0.5).\n",
    "        \"\"\"\n",
    "        image_size = example_size[0]\n",
    "        num_channels = example_size[-1]\n",
    "               \n",
    "        self.sess = sess\n",
    "        self.TARGETED = targeted\n",
    "        self.LEARNING_RATE = learning_rate\n",
    "        self.MAX_ITERATIONS = max_iterations\n",
    "        self.BINARY_SEARCH_STEPS = binary_search_steps\n",
    "        self.ABORT_EARLY = abort_early\n",
    "        self.CONFIDENCE = confidence\n",
    "        self.initial_const = initial_const\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.repeat = binary_search_steps >= 10\n",
    "\n",
    "        shape = (batch_size,image_size,image_size,num_channels)\n",
    "        \n",
    "        # the variable we're going to optimize over\n",
    "        modifier = tf.Variable(np.zeros(shape,dtype=np.float32))\n",
    "\n",
    "        # these are variables to be more efficient in sending data to tf\n",
    "        self.timg = tf.Variable(np.zeros(shape), dtype=tf.float32)\n",
    "        self.tlab = tf.Variable(np.zeros((batch_size,num_labels)), dtype=tf.float32)\n",
    "        self.const = tf.Variable(np.zeros(batch_size), dtype=tf.float32)\n",
    "\n",
    "        # and here's what we use to assign them\n",
    "        self.assign_timg = tf.placeholder(tf.float32, shape)\n",
    "        self.assign_tlab = tf.placeholder(tf.float32, (batch_size,num_labels))\n",
    "        self.assign_const = tf.placeholder(tf.float32, [batch_size])\n",
    "        \n",
    "        # the resulting image, tanh'd to keep bounded from boxmin to boxmax\n",
    "        self.boxmul = (boxmax - boxmin) / 2.\n",
    "        self.boxplus = (boxmin + boxmax) / 2.\n",
    "        self.newimg = tf.tanh(modifier + self.timg) * self.boxmul + self.boxplus\n",
    "        \n",
    "        # prediction BEFORE-SOFTMAX of the model\n",
    "        self.output = model(self.newimg)\n",
    "        print(self.newimg.shape)\n",
    "        print(self.output.shape)\n",
    "        print(type(self.output))\n",
    "        # distance to the input data\n",
    "        self.l2dist = tf.reduce_sum(tf.square(self.newimg-(tf.tanh(self.timg) * self.boxmul + self.boxplus)),[1,2,3])\n",
    "        \n",
    "        # compute the probability of the label class versus the maximum other\n",
    "        real = tf.reduce_sum((self.tlab)*self.output,1)\n",
    "        other = tf.reduce_max((1-self.tlab)*self.output - (self.tlab*10000),1)\n",
    "\n",
    "        if self.TARGETED:\n",
    "            # if targetted, optimize for making the other class most likely\n",
    "            loss1 = tf.maximum(0.0, other-real+self.CONFIDENCE)\n",
    "        else:\n",
    "            # if untargeted, optimize for making this class least likely.\n",
    "            loss1 = tf.maximum(0.0, real-other+self.CONFIDENCE)\n",
    "\n",
    "        # sum up the losses\n",
    "        self.loss2 = tf.reduce_sum(self.l2dist)\n",
    "        self.loss1 = tf.reduce_sum(self.const*loss1)\n",
    "        self.loss = self.loss1+self.loss2\n",
    "        \n",
    "        # Setup the adam optimizer and keep track of variables we're creating\n",
    "        start_vars = set(x.name for x in tf.global_variables())\n",
    "        optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE)\n",
    "        self.train = optimizer.minimize(self.loss, var_list=[modifier])\n",
    "        end_vars = tf.global_variables()\n",
    "        new_vars = [x for x in end_vars if x.name not in start_vars]\n",
    "        \n",
    "        # these are the variables to initialize when we run\n",
    "        self.setup = []\n",
    "        self.setup.append(self.timg.assign(self.assign_timg))\n",
    "        self.setup.append(self.tlab.assign(self.assign_tlab))\n",
    "        self.setup.append(self.const.assign(self.assign_const))\n",
    "        \n",
    "        self.init = tf.variables_initializer(var_list=[modifier]+new_vars)\n",
    "\n",
    "    def attack(self, imgs, targets):\n",
    "        \"\"\"\n",
    "        Perform the L_2 attack on the given images for the given targets.\n",
    "\n",
    "        If self.targeted is true, then the targets represents the target labels.\n",
    "        If self.targeted is false, then targets are the original class labels.\n",
    "        \"\"\"\n",
    "        r = []\n",
    "        print('go up to',len(imgs))\n",
    "        for i in range(0,len(imgs),self.batch_size):\n",
    "            print('tick',i)\n",
    "            r.extend(self.attack_batch(imgs[i:i+self.batch_size], targets[i:i+self.batch_size]))\n",
    "        return np.array(r)\n",
    "\n",
    "    def attack_batch(self, imgs, labs):\n",
    "        \"\"\"\n",
    "        Run the attack on a batch of images and labels.\n",
    "        \"\"\"\n",
    "        def compare(x,y):\n",
    "            if not isinstance(x, (float, int, np.int64)):\n",
    "                x = np.copy(x)\n",
    "                if self.TARGETED:\n",
    "                    x[y] -= self.CONFIDENCE\n",
    "                else:\n",
    "                    x[y] += self.CONFIDENCE\n",
    "                x = np.argmax(x)\n",
    "            if self.TARGETED:\n",
    "                return x == y\n",
    "            else:\n",
    "                return x != y\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        # convert to tanh-space\n",
    "        imgs = np.arctanh((imgs - self.boxplus) / self.boxmul * 0.999999)\n",
    "\n",
    "        # set the lower and upper bounds accordingly\n",
    "        lower_bound = np.zeros(batch_size)\n",
    "        CONST = np.ones(batch_size)*self.initial_const\n",
    "        upper_bound = np.ones(batch_size)*1e10\n",
    "\n",
    "        # the best l2, score, and image attack\n",
    "        o_bestl2 = [1e10]*batch_size\n",
    "        o_bestscore = [-1]*batch_size\n",
    "        o_bestattack = [np.zeros(imgs[0].shape)]*batch_size\n",
    "        \n",
    "        for outer_step in range(self.BINARY_SEARCH_STEPS):\n",
    "            print(o_bestl2)\n",
    "            # completely reset adam's internal state.\n",
    "            self.sess.run(self.init)\n",
    "            \n",
    "            #Take in the current batch.\n",
    "            batch = imgs[:batch_size]\n",
    "            batchlab = labs[:batch_size]\n",
    "    \n",
    "            bestl2 = [1e10]*batch_size\n",
    "            bestscore = [-1]*batch_size\n",
    "\n",
    "            # The last iteration (if we run many steps) repeat the search once.\n",
    "            if self.repeat == True and outer_step == self.BINARY_SEARCH_STEPS-1:\n",
    "                CONST = upper_bound\n",
    "\n",
    "            # set the variables so that we don't have to send them over again\n",
    "            self.sess.run(self.setup, {self.assign_timg: batch,\n",
    "                                       self.assign_tlab: batchlab,\n",
    "                                       self.assign_const: CONST})\n",
    "            \n",
    "            prev = 1e6\n",
    "            for iteration in range(self.MAX_ITERATIONS):\n",
    "                # perform the attack \n",
    "                _, l, l2s, scores, nimg = self.sess.run([self.train, self.loss,\n",
    "                                                         self.l2dist, self.output,\n",
    "                                                         self.newimg])\n",
    "\n",
    "                # print out the losses every 10%\n",
    "                if iteration%(self.MAX_ITERATIONS//10) == 0:\n",
    "                    print(iteration,self.sess.run((self.loss,self.loss1,self.loss2)))\n",
    "\n",
    "                # check if we should abort search if we're getting nowhere.\n",
    "                if self.ABORT_EARLY and iteration%(self.MAX_ITERATIONS//10) == 0:\n",
    "                    if l > prev*.9999:\n",
    "                        break\n",
    "                    prev = l\n",
    "\n",
    "                # adjust the best result found so far\n",
    "                for e,(l2,sc,ii) in enumerate(zip(l2s,scores,nimg)):\n",
    "                    if l2 < bestl2[e] and compare(sc, np.argmax(batchlab[e])):\n",
    "                        bestl2[e] = l2\n",
    "                        bestscore[e] = np.argmax(sc)\n",
    "                    if l2 < o_bestl2[e] and compare(sc, np.argmax(batchlab[e])):\n",
    "                        o_bestl2[e] = l2\n",
    "                        o_bestscore[e] = np.argmax(sc)\n",
    "                        o_bestattack[e] = ii\n",
    "\n",
    "            # adjust the constant as needed\n",
    "            for e in range(batch_size):\n",
    "                if compare(bestscore[e], np.argmax(batchlab[e])) and bestscore[e] != -1:\n",
    "                    # success, divide const by two\n",
    "                    upper_bound[e] = min(upper_bound[e],CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e])/2\n",
    "                else:\n",
    "                    # failure, either multiply by 10 if no solution found yet\n",
    "                    #          or do binary search with the known upper bound\n",
    "                    lower_bound[e] = max(lower_bound[e],CONST[e])\n",
    "                    if upper_bound[e] < 1e9:\n",
    "                        CONST[e] = (lower_bound[e] + upper_bound[e])/2\n",
    "                    else:\n",
    "                        CONST[e] *= 10\n",
    "\n",
    "        # return the best solution found\n",
    "        return np.stack(o_bestattack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"1_2\"></a>\n",
    "\n",
    "## 1.2  Testing C&W\n",
    "\n",
    "We launch the C&W attack against a MNIST classifier in this section. We reuse `mnist_cnn_model.h5` as the target classifier. Let us first import the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded, training x:(60000, 28, 28, 1), y:(10000, 28, 28, 1); test x:(60000, 10), y(10000, 10).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# we normalise the pixels to values between 0 and 1\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = np.float32(X_train/255.)\n",
    "X_test = np.float32(X_test/255.)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "print('data loaded, training x:{0}, y:{1}; test x:{2}, y{3}.'.format(X_train.shape,\n",
    "                                                                     X_test.shape,\n",
    "                                                                     y_train.shape,\n",
    "                                                                     y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us instantiate the `CW_Attack` class and start the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate the loaded model\n",
      "10000/10000 [==============================] - 1s 93us/step\n",
      "Accuracy of the loaded model: 0.9916\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Normal_inputs (InputLayer)   (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "C1 (Conv2D)                  (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "C2 (Conv2D)                  (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "MP1 (MaxPooling2D)           (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "D1 (Dense)                   (None, 200)               1254600   \n",
      "_________________________________________________________________\n",
      "logits (Dense)               (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 1,266,178\n",
      "Trainable params: 1,266,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(1, 28, 28, 1)\n",
      "(1, 10)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "[10000000000.0]\n",
      "0 (0.0009999999, 0.0009999999, 9.070966e-12)\n",
      "1000 (0.001000305, 0.0009999999, 3.051768e-07)\n",
      "[10000000000.0]\n",
      "0 (0.01, 0.009999999, 9.2096997e-10)\n",
      "1000 (0.010000158, 0.009999999, 1.5971885e-07)\n",
      "[10000000000.0]\n",
      "0 (0.10000026, 0.099999994, 2.6851046e-07)\n",
      "1000 (0.099999994, 0.099999994, 3.0404665e-09)\n",
      "[10000000000.0]\n",
      "0 (1.000014, 0.99999994, 1.4035226e-05)\n",
      "1000 (0.99999994, 0.99999994, 1.7491297e-09)\n",
      "[10000000000.0]\n",
      "0 (10.000158, 9.999999, 0.00015894165)\n",
      "1000 (9.999999, 9.999999, 8.875777e-08)\n",
      "[10000000000.0]\n",
      "0 (100.00074, 99.99999, 0.00074730883)\n",
      "1000 (99.99999, 99.99999, 5.171592e-09)\n",
      "[10000000000.0]\n",
      "0 (1000.001, 999.99994, 0.0010134524)\n",
      "1000 (999.99994, 999.99994, 2.6076464e-07)\n",
      "[10000000000.0]\n",
      "0 (10000.0, 9999.999, 0.0010637969)\n",
      "1000 (9999.999, 9999.999, 1.7881986e-05)\n",
      "[10000000000.0]\n",
      "0 (99999.99, 99999.99, 0.0010804402)\n",
      "1000 (4.8584557, 0.0, 4.8584557)\n",
      "2000 (4.7804394, 0.0, 4.7804394)\n",
      "3000 (4.645096, 0.0, 4.645096)\n",
      "4000 (4.428595, 0.0, 4.428595)\n",
      "5000 (4.10166, 0.0, 4.10166)\n",
      "6000 (3.6415427, 0.0, 3.6415427)\n",
      "7000 (3.052404, 0.0, 3.052404)\n",
      "8000 (5.9211106, 0.0, 5.9211106)\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "target_classifier = load_model('mnist_cnn_model.h5')\n",
    "print('Evaluate the loaded model')\n",
    "print(f'Accuracy of the loaded model: {target_classifier.evaluate(X_test, y_test)[1]}')\n",
    "target_classifier.layers.pop()\n",
    "target_classifier.summary()\n",
    "\n",
    "# instantiate an attacker\n",
    "cw_attacker = CW_Attack(sess, target_classifier, example_size=X_train[0].shape, batch_size=1)\n",
    "\n",
    "benign_example = X_test[4:5]\n",
    "bening_y = y_test[4:5]\n",
    "cw_x = cw_attacker.attack_batch(benign_example, bening_y)\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualise the generated C&W example using Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(cw_x.shape)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "axes[0].imshow(benign_example.reshape(28,28), cmap='gray')\n",
    "axes[1].imshow(cw_x.reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the classifier predictions on the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier prediction on the benign example: 4\n",
      "Keras classifier prediction on the fgsm example: 9\n"
     ]
    }
   ],
   "source": [
    "# Input the examples into the classifier for classification\n",
    "print(f'Keras classifier prediction on the benign example: {np.argmax(target_classifier.predict(benign_example))}')\n",
    "print(f'Keras classifier prediction on the C&W example: {np.argmax(target_classifier.predict(cw_x))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the CW perturbation is nearly imperceptible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2.  Gradient-free Particle Swarm Optimisation attack (Not mandatary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"2_1\"></a>\n",
    "\n",
    "## 2.1  Implementing the PSO attack\n",
    "\n",
    "Pasrticle Swarm Optimisation is a gradient-free (zeroth-order) optimisation method. It does not need to compute the gradients of a target model. Instead, it only query the target model to get model outputs. Base on the outputs, PSO can adjust the example to be adversarial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "\n",
    "'''\n",
    "# recommended params for attacking:\n",
    "Global: momentum=0.5, c1=0.5, c2=0.5\n",
    "    cifar10 : DELTA=0.1, SAM_RATE=0.003, 1440 queries (low perturbation)\n",
    "    mnist \t: DELTA=0.1, SAM_RATE=0.005, 0~160~240 queries (fast)\n",
    "\n",
    "'''\n",
    "\n",
    "VICTIM = 'mnist'\n",
    "INPUT_SIZE = 28\n",
    "INPUT_CHANNEL = 1\n",
    "CLASS_NUM = 10\n",
    "CONFIDENCE = 0.1\n",
    "DELTA = 0.1\n",
    "SAM_RATE = 0.005 # The sampling ratio of features for perturbing in bold particles\n",
    "SPIKE_VALUE = 1 # Spike values assigned to the sampled features in the bold particles\n",
    "TARGETED = False\n",
    "TARGET = np.eye(10, dtype=int)[0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next,let us generate FGSM examples of the trainig dataset on the `mnist_cnn_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSOAttack:\n",
    "    def __init__(self, sess, model, confidence=CONFIDENCE, targeted=TARGETED, particle_num=20, sampling_rate=SAM_RATE,\n",
    "                 spike_value=SPIKE_VALUE, momentum=0.5, c1=0.5, c2=0.5, delta=DELTA, epsilon=1, max_iter=100, max_epochs=10,\n",
    "                 class_num=CLASS_NUM, input_size=INPUT_SIZE, input_channel=INPUT_CHANNEL):\n",
    "\n",
    "        def nontargeted_cost(positions_labs):\n",
    "            (position, tlab) = positions_labs\n",
    "            outputs = self.model(position)\n",
    "            top_k_values, top_k_inds = tf.math.top_k(outputs, k=class_num)\n",
    "            top1_ind = tf.cast(tf.reduce_sum(top_k_inds[:,0]), tf.int64) # labels with the highest confidence score, scalar\n",
    "            top1_conf = tf.reduce_sum(top_k_values[:,0])\n",
    "            least_ind = tf.cast(tf.reduce_sum(top_k_inds[:,class_num-1]), tf.int64)\n",
    "            least_conf = tf.reduce_sum(top_k_values[:,class_num-1])\n",
    "            target_rank = tf.reduce_sum(tf.where(tf.equal(tf.argmax(tlab, -1), tf.cast(top_k_inds, tf.int64)))[:,1])\n",
    "            target_conf = tf.reduce_sum(tlab * outputs)\n",
    "            other_conf = tf.reduce_max((1-tlab)*outputs - (tlab*10000))\n",
    "            def fn_true():\n",
    "                return tf.identity(1e-100)\n",
    "            def fn_false():\n",
    "                #return (target_conf+self.confidence)/tf.cast(target_rank+1, tf.float32)\n",
    "                return tf.maximum(0.0, target_conf-other_conf+self.confidence)\n",
    "            return tf.cond(tf.not_equal(target_rank, 0), fn_true, fn_false), target_rank, top_k_inds\n",
    "\n",
    "        def targeted_cost(positions_labs):\n",
    "            (position, tlab) = positions_labs\n",
    "            outputs = self.model(position)\n",
    "            top_k_values, top_k_inds = tf.math.top_k(outputs, k=class_num)\n",
    "            top1_ind = tf.cast(tf.reduce_sum(top_k_inds[:,0]), tf.int64) # labels with the highest confidence score, scalar\n",
    "            top1_conf = tf.reduce_sum(top_k_values[:,0])\n",
    "            least_ind = tf.cast(tf.reduce_sum(top_k_inds[:,class_num-1]), tf.int64)\n",
    "            least_conf = tf.reduce_sum(top_k_values[:,class_num-1])\n",
    "            target_rank = tf.reduce_sum(tf.where(tf.equal(tf.argmax(tlab, -1), tf.cast(top_k_inds, tf.int64)))[:,1])\n",
    "            target_conf = tf.reduce_sum(tlab * outputs)\n",
    "            def fn_true():\n",
    "                return tf.identity(1e-100)\n",
    "            def fn_false():\n",
    "                return tf.cast(target_rank+1, tf.float32)*(top1_conf+self.confidence)/target_conf\n",
    "            return tf.cond(tf.equal(target_rank, 0), fn_true, fn_false), target_rank, top_k_inds\n",
    "\n",
    "        def sample_mask(pdf, s, n, replace):\n",
    "            \"\"\"Initialize the model.\n",
    "\n",
    "                Args:    \n",
    "                     pdf: A 3D Tensor of shape (batch_size, hight, width, channels=1) to use as a PDF\n",
    "                     s: The number of samples per mask. This value should be less than hight*width\n",
    "                     n: The total number of masks to generate\n",
    "                     replace: A boolean indicating if sampling should be done with replacement\n",
    "\n",
    "                Returns:\n",
    "                    A Tensor of shape (batch_size, hight, width, channels=1, n) containing\n",
    "                    values 1 or 0.\n",
    "            \"\"\"\n",
    "            def sample_without_replacement(logits, K):\n",
    "                z = -tf.log(-tf.log(tf.random_uniform(tf.shape(logits),0,1)))\n",
    "                _, indices = tf.nn.top_k(logits + z, K)\n",
    "                return indices\n",
    "\n",
    "            batch_size, _, hight, width, channels = pdf.shape\n",
    "            # Flatten pdf\n",
    "            pdf = tf.reshape(pdf, (batch_size, hight*width*channels))\n",
    "\n",
    "            if replace:\n",
    "                # Sample with replacement. Output is a tensor of shape (batch_size, n)\n",
    "                sample_fun = lambda: tf.multinomial(tf.log(pdf), s)\n",
    "            else:\n",
    "                # Sample without replacement. Output is a tensor of shape (batch_size, n).\n",
    "                # Cast the output to 'int64' to match the type needed for SparseTensor's indices\n",
    "                sample_fun = lambda: tf.cast(sample_without_replacement(tf.log(pdf), s), dtype='int64')\n",
    "\n",
    "            # Create batch indices\n",
    "            idx = tf.range(batch_size, dtype='int64')\n",
    "            idx = tf.expand_dims(idx, 1)\n",
    "            # Transform idx to a 2D tensor of shape (batch_size, samples_per_batch)\n",
    "            # Example: [[0 0 0 0 0],[1 1 1 1 1],[2 2 2 2 2]]\n",
    "            idx = tf.tile(idx, [1, s])\n",
    "\n",
    "            mask_list = []\n",
    "            for i in range(n):\n",
    "                # Generate samples\n",
    "                samples = sample_fun()\n",
    "                # Combine batch indices and samples\n",
    "                samples = tf.stack([idx,samples])\n",
    "                # Transform samples to a list of indicies: (batch_index, sample_index)\n",
    "                sample_indices = tf.transpose(tf.reshape(samples, [2, -1]))\n",
    "                # Create the mask as a sparse tensor and set sampled indices to 1\n",
    "                mask = tf.SparseTensor(indices=sample_indices, values=tf.ones(s*batch_size), dense_shape=[batch_size, hight*width*channels]) \n",
    "                # Convert mask to a dense tensor. Non-sampled values are set to 0.\n",
    "                # Don't validate the indices, since this requires indices to be ordered\n",
    "                # and unique.\n",
    "                mask = tf.sparse.to_dense(mask, default_value=0,validate_indices=False)\n",
    "                # Reshape to input shape and append to list of tensors\n",
    "                mask_list.append(tf.reshape(mask, [batch_size, 1, hight, width, channels]))\n",
    "            # Combine all masks into a tensor of shape:\n",
    "            # (batch_size, hight, width, channels=1, number_of_masks)\n",
    "            return tf.concat(mask_list, axis=0)\n",
    "\n",
    "        self.sess = sess\n",
    "        self.momentum = momentum\n",
    "        self.max_iter = max_iter\n",
    "        self.max_epochs = max_epochs\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.particle_num = particle_num\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.spike_value = spike_value\n",
    "        self.confidence = confidence\n",
    "        self.model = model\n",
    "        self.targeted = targeted\n",
    "\n",
    "        # Placeholders\n",
    "        self.assign_inputs = tf.placeholder(tf.float32, (1, input_size, input_size, input_channel), name='inputs')\n",
    "        self.assign_tlab = tf.placeholder(tf.float32, (1, class_num), name='tlab')\n",
    "\n",
    "        # define variables to use\n",
    "        self.single_input = tf.Variable(np.zeros((1, input_size, input_size, input_channel)), dtype=tf.float32, name='single_input')\n",
    "        self.single_tlab = tf.Variable(np.zeros((1, class_num)), dtype=tf.float32, name='single_tlab')\n",
    "        self.positions = tf.Variable(np.zeros((self.particle_num, 1, input_size, input_size, input_channel)), dtype=tf.float32, name='positions')\n",
    "        self.velocities = tf.Variable(np.zeros((self.particle_num, 1, input_size, input_size, input_channel)), dtype=tf.float32, name='velocities')\n",
    "        self.self_bests = tf.Variable(np.zeros((self.particle_num, 1, input_size, input_size, input_channel)), dtype=tf.float32, name='self_bests')\n",
    "        self.neighbour_best = tf.Variable(np.zeros((1, input_size, input_size, input_channel)), dtype=tf.float32, name='neighbour_best')\n",
    "        self.global_best = tf.Variable(np.zeros((1, input_size, input_size, input_channel)), dtype=tf.float32, name='global_best')\n",
    "\n",
    "        # input op for tf\n",
    "        self.duplicate_to_particles = tf.stack([self.single_input]*self.particle_num, name='duplicate_to_particles') # (20, 1, 28, 28, 1)\n",
    "        self.tlabs = tf.stack([self.single_tlab]*self.particle_num, name='tlabs') # (20, 1, class_num)\n",
    "\n",
    "        # distortion constraints bounded by delta\n",
    "        dist_upper_bound = tf.minimum(self.duplicate_to_particles + delta, 1)\n",
    "        dist_lower_bound = tf.maximum(self.duplicate_to_particles - delta, 0)\n",
    "\n",
    "        # cost function for optimising, here we only query the target model!\n",
    "        if self.targeted:\n",
    "            self.best_costs, self.best_confs, self.best_ranking = tf.map_fn(targeted_cost, (self.self_bests, self.tlabs), dtype=(tf.float32, tf.int64, tf.int32))\n",
    "            self.current_costs, self.current_confs, self.current_ranking = tf.map_fn(targeted_cost, (self.positions, self.tlabs), dtype=(tf.float32, tf.int64, tf.int32))\n",
    "        else:\n",
    "            self.best_costs, self.best_confs, self.best_ranking = tf.map_fn(nontargeted_cost, (self.self_bests, self.tlabs), dtype=(tf.float32, tf.int64, tf.int32)) # loss values for each particles in shape (20, 1)\n",
    "            self.current_costs, self.current_confs, self.current_ranking = tf.map_fn(nontargeted_cost, (self.positions, self.tlabs), dtype=(tf.float32, tf.int64, tf.int32)) # loss values for each particles in shape (20, 1)\n",
    "        self.best_neighbour_ind = tf.reduce_sum(tf.argmin(self.current_costs))\n",
    "        self.best_global_ind = tf.reduce_sum(tf.argmin(self.best_costs))\n",
    "\n",
    "        # feeding inputs into the graph\n",
    "        self.setup = []\n",
    "        self.setup.append(self.single_input.assign(self.assign_inputs))\n",
    "        self.setup.append(self.single_tlab.assign(self.assign_tlab))\n",
    "\n",
    "        # Allowed sparse high-level perturbation for bold particles.\n",
    "        print(self.positions.shape)\n",
    "        self.pos_masks = sample_mask(self.positions,\n",
    "                                     s=tf.cast(0.5*self.sampling_rate*(input_size**2), tf.int32),\n",
    "                                     n=1,\n",
    "                                     replace=False)\n",
    "        self.neg_masks = sample_mask(self.positions,\n",
    "                                     s=tf.cast(0.5*self.sampling_rate*(input_size**2), tf.int32),\n",
    "                                     n=1,\n",
    "                                     replace=False)\n",
    "        self.spikes = spike_value*self.pos_masks - spike_value*self.neg_masks # produce a sparse perturbation of -1 and 1\n",
    "\n",
    "        # initilize positions, velocities, and self/neighbour bests.\n",
    "        self.initilize = []\n",
    "        self.initilize.append(self.positions.assign(tf.clip_by_value(self.duplicate_to_particles + self.spikes , dist_lower_bound, dist_upper_bound)))\n",
    "        self.initilize.append(self.velocities.assign(tf.zeros((self.particle_num, 1, input_size, input_size, input_channel), dtype=tf.float32)))\n",
    "        self.initilize.append(self.self_bests.assign(self.duplicate_to_particles))\n",
    "        self.initilize.append(self.neighbour_best.assign(self.duplicate_to_particles[self.best_neighbour_ind]))\n",
    "        self.initilize.append(self.global_best.assign(self.self_bests[self.best_global_ind]))\n",
    "\n",
    "        # updates in each iteration\n",
    "        r1 = tf.random.uniform(tf.shape(self.positions), 0, self.c1)\n",
    "        r2 = tf.random.uniform(tf.shape(self.positions), 0, self.c2)\n",
    "\n",
    "        self.update_positions = tf.clip_by_value(self.positions + self.spikes + epsilon*self.velocities, dist_lower_bound, dist_upper_bound)\n",
    "        self.update_velocities = self.momentum*self.velocities + r1*(self.self_bests-self.positions) + r2*(self.neighbour_best-self.positions)\n",
    "\n",
    "        update_ind = tf.where(tf.less_equal(self.current_costs, self.best_costs))[:,0] # (?,)\n",
    "        self.update_self_bests = tf.scatter_update(self.self_bests, update_ind, tf.gather(self.positions, update_ind))\n",
    "        self.update_neighbour_best = self.positions[self.best_neighbour_ind]\n",
    "        self.update_global_best = self.self_bests[self.best_global_ind]\n",
    "\n",
    "        self.updates1 = [self.positions.assign(self.update_positions),\n",
    "                         self.velocities.assign(self.update_velocities)]\n",
    "\n",
    "        self.updates2 = [self.update_self_bests,\n",
    "                         self.neighbour_best.assign(self.update_neighbour_best)]\n",
    "\n",
    "        self.updates3 = [self.global_best.assign(self.update_global_best)]\n",
    "\n",
    "        # op for initialising variables\n",
    "        self.init = tf.variables_initializer(var_list=[self.single_input, self.single_tlab, self.positions, self.velocities,\n",
    "                                                       self.self_bests, self.neighbour_best, self.global_best])\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "    def optimise(self, data_x, data_y):\n",
    "        '''\n",
    "            data_x : single input feature tensor in shape (1, 28, 28, 1)\n",
    "            data_y : single input feature tensor in shape (1, 28, 28, 1)\n",
    "        '''\n",
    "        #print('target is: ', data_y)\n",
    "        #print(self.model.evaluate(test_x, test_y))\n",
    "        self.sess.run(self.setup + self.initilize, {self.assign_inputs: data_x, self.assign_tlab: data_y})\n",
    "        # Obtain initial values\n",
    "        velocities, positions, sb, nb, gb, loss, b_loss, best_ind, target_rank, top_k_inds = self.sess.run([self.velocities,\n",
    "                                                                                                            self.positions,\n",
    "                                                                                                            self.self_bests,\n",
    "                                                                                                            self.neighbour_best,\n",
    "                                                                                                            self.global_best,\n",
    "                                                                                                            self.current_costs,\n",
    "                                                                                                            self.best_costs,\n",
    "                                                                                                            self.best_neighbour_ind,\n",
    "                                                                                                            self.current_confs,\n",
    "                                                                                                            self.current_ranking])\n",
    "        print(positions.shape, gb.shape)\n",
    "\n",
    "        query_num = 2*self.particle_num\n",
    "        for epoch in range(self.max_epochs):\n",
    "            print('Epoch: {0}'.format(epoch))\n",
    "            # Initilaise seeds\n",
    "            self.sess.run(self.setup + self.initilize, {self.assign_inputs: gb, self.assign_tlab: data_y})\n",
    "            # Obtain initial values\n",
    "            velocities, positions, sb, nb, gb, loss, b_loss, best_ind, target_rank, top_k_inds = self.sess.run([self.velocities,\n",
    "                                                                                                                self.positions,\n",
    "                                                                                                                self.self_bests,\n",
    "                                                                                                                self.neighbour_best,\n",
    "                                                                                                                self.global_best,\n",
    "                                                                                                                self.current_costs,\n",
    "                                                                                                                self.best_costs,\n",
    "                                                                                                                self.best_neighbour_ind,\n",
    "                                                                                                                self.current_confs,\n",
    "                                                                                                                self.current_ranking])\n",
    "            print('Start optimisation: current loss is: {0}, and the best loss is {1}, and the best among neighbours is {2}.'.format(loss, b_loss, best_ind))\n",
    "            print('the rankings of the classification target are: {0}. The top 10 ranking is: {1}.'.format(target_rank, top_k_inds))\n",
    "            p_m = sess.run(self.pos_masks)\n",
    "            n_m = sess.run(self.neg_masks)\n",
    "            #print('Positive mask is: {0}, number of spikes is: {1}. Number of negative spike is: {2}.'.format(p_m, np.sum(p_m, dtype=np.float32), np.sum(n_m, dtype=np.float32)))\n",
    "            #print('\\nCurrent velocities are:{0}'.format(velocities))\n",
    "            for i in range(self.max_iter):\n",
    "                self.sess.run(self.updates1)\n",
    "                self.sess.run(self.updates2)\n",
    "                self.sess.run(self.updates3)\n",
    "                velocities, positions, sb, nb, gb, loss, b_loss, best_ind, target_rank, top_k_inds = self.sess.run([self.velocities,\n",
    "                                                                                                                    self.positions,\n",
    "                                                                                                                    self.self_bests,\n",
    "                                                                                                                    self.neighbour_best,\n",
    "                                                                                                                    self.global_best,\n",
    "                                                                                                                    self.current_costs,\n",
    "                                                                                                                    self.best_costs,\n",
    "                                                                                                                    self.best_neighbour_ind,\n",
    "                                                                                                                    self.current_confs,\n",
    "                                                                                                                    self.current_ranking])\n",
    "                if i%100 == 0:\n",
    "                    print('\\nIteration {0}: current loss is: {1}, the best loss is {2}, and the best among neighbours is {3}.'.format(i, loss, b_loss, best_ind))\n",
    "                    print('the rankings of the classification target are: {0}. The top 10 ranking is: {1}.'.format(target_rank, top_k_inds))\n",
    "                    #print('\\nCurrent velocities are:{0}'.format(velocities))\n",
    "                \n",
    "                # return examples when at least one valid adversarial example is found\n",
    "                if self.targeted:\n",
    "                    if 0 in target_rank:\n",
    "                        query_num = (epoch*self.max_iter + i + 1)*2*self.particle_num\n",
    "                        print('Adversarial example has been found in the {0}-th query!!!'.format(query_num))\n",
    "                        print('\\nIteration {0}: current loss is: {1}, the best loss is {2}, and the best among neighbours is {3}.'.format(i, loss, b_loss, best_ind))\n",
    "                        print('the rankings of the classification target are: {0}. The top 10 ranking is: {1}.'.format(target_rank, top_k_inds))\n",
    "                        adv_ind = np.where(target_rank == 0)\n",
    "                        return positions[adv_ind]\n",
    "                else:\n",
    "                    if np.sum(target_rank) > 0:\n",
    "                        query_num = (epoch*self.max_iter + i)*2*self.particle_num\n",
    "                        print('Adversarial example has been found in the {0}-th query!!!'.format(query_num))\n",
    "                        print('\\nIteration {0}: current loss is: {1}, the best loss is {2}, and the best among neighbours is {3}.'.format(i, loss, b_loss, best_ind))\n",
    "                        print('the rankings of the classification target are: {0}. The top 10 ranking is: {1}.'.format(target_rank, top_k_inds))\n",
    "                        adv_ind = np.where(target_rank != 0)\n",
    "                        return positions[adv_ind]\n",
    "        print('Cannot found anything!!!')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1, 28, 28, 1)\n",
      "(20, 1, 28, 28, 1) (1, 28, 28, 1)\n",
      "Epoch: 0\n",
      "Start optimisation: current loss is: [1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999\n",
      " 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999\n",
      " 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999], and the best loss is [1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999\n",
      " 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999\n",
      " 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999], and the best among neighbours is 0.\n",
      "the rankings of the classification target are: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]. The top 10 ranking is: [[[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 2 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 2 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 6 0 2 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 6 0 2 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 6 2 0 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 6 0 2 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 6 2 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 6 2 0 5 8 1 3]]].\n",
      "\n",
      "Iteration 0: current loss is: [1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999\n",
      " 1.0999999 1.0999998 1.0999999 1.0999999 1.1       1.0999999 1.0999999\n",
      " 1.0999999 1.0999999 1.0999999 1.0999998 1.0999999 1.0999999], the best loss is [1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999\n",
      " 1.0999999 1.0999998 1.0999999 1.0999999 1.0999999 1.0999999 1.0999999\n",
      " 1.0999999 1.0999999 1.0999999 1.0999998 1.0999999 1.0999999], and the best among neighbours is 8.\n",
      "the rankings of the classification target are: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]. The top 10 ranking is: [[[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 2 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 6 0 2 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 6 0 2 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 6 0 2 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 6 2 0 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 6 2 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 6 0 2 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 2 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 2 0 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 2 6 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 6 2 5 8 1 3]]\n",
      "\n",
      " [[4 9 7 6 0 2 5 8 1 3]]].\n",
      "Epoch: 1\n",
      "Start optimisation: current loss is: [1.099828  1.0998312 1.0998379 1.0997957 1.0998328 1.099867  1.0998844\n",
      " 1.099842  1.0998844 1.0998194 1.0998305 1.0998409 1.0998567 1.0997964\n",
      " 1.0998324 1.0998737 1.0998207 1.0998805 1.0998322 1.0997986], and the best loss is [1.0998348 1.0998348 1.0998348 1.0998348 1.0998348 1.0998348 1.0998348\n",
      " 1.0998348 1.0998348 1.0998348 1.0998348 1.0998348 1.0998348 1.0998348\n",
      " 1.0998348 1.0998348 1.0998348 1.0998348 1.0998348 1.0998348], and the best among neighbours is 3.\n",
      "the rankings of the classification target are: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]. The top 10 ranking is: [[[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 6 2 8 1 3]]].\n",
      "\n",
      "Iteration 0: current loss is: [1.0998319 1.0998409 1.0998172 1.0998133 1.0998336 1.099882  1.0998875\n",
      " 1.0998586 1.0998629 1.0998036 1.0997659 1.0998024 1.0998576 1.0997814\n",
      " 1.0998045 1.0998865 1.0998307 1.0998955 1.099792  1.0998045], the best loss is [1.0998319 1.0998348 1.0998172 1.0998133 1.0998336 1.0998348 1.0998348\n",
      " 1.0998348 1.0998348 1.0998036 1.0997659 1.0998024 1.0998348 1.0997814\n",
      " 1.0998045 1.0998348 1.0998307 1.0998348 1.099792  1.0998045], and the best among neighbours is 10.\n",
      "the rankings of the classification target are: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]. The top 10 ranking is: [[[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 6 2 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 2 6 8 1 3]]\n",
      "\n",
      " [[4 9 7 0 5 6 2 8 1 3]]].\n",
      "Epoch: 2\n",
      "Start optimisation: current loss is: [0.9756539  0.9670868  0.9764268  0.9690137  0.9839456  0.9864213\n",
      " 0.96753615 0.98031414 0.9911986  0.9814531  0.9764268  0.9893589\n",
      " 1.0021778  0.9760107  0.97825015 0.9313137  1.0148137  0.9621191\n",
      " 1.0101855  0.9797793 ], and the best loss is [0.9797394 0.9797394 0.9797394 0.9797394 0.9797394 0.9797394 0.9797394\n",
      " 0.9797394 0.9797394 0.9797394 0.9797394 0.9797394 0.9797394 0.9797394\n",
      " 0.9797394 0.9797394 0.9797394 0.9797394 0.9797394 0.9797394], and the best among neighbours is 15.\n",
      "the rankings of the classification target are: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]. The top 10 ranking is: [[[4 9 7 5 8 0 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 8 0 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 8 0 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]].\n",
      "\n",
      "Iteration 0: current loss is: [0.9768094  0.97353315 0.95142615 0.97126114 0.9816204  0.9830586\n",
      " 0.9607405  0.9970277  1.0035769  0.9557531  0.9472285  1.0100951\n",
      " 1.0248337  0.9976942  0.9854347  0.8784822  1.0071727  0.9793794\n",
      " 1.0152731  1.0117618 ], the best loss is [0.9768094  0.97353315 0.95142615 0.97126114 0.9797394  0.9797394\n",
      " 0.9607405  0.9797394  0.9797394  0.9557531  0.9472285  0.9797394\n",
      " 0.9797394  0.9797394  0.9797394  0.8784822  0.9797394  0.9793794\n",
      " 0.9797394  0.9797394 ], and the best among neighbours is 15.\n",
      "the rankings of the classification target are: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]. The top 10 ranking is: [[[4 9 7 5 8 0 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 8 0 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 8 0 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 8 0 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 8 0 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 8 0 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]].\n",
      "Adversarial example has been found in the 9000-th query!!!\n",
      "\n",
      "Iteration 25: current loss is: [0.1999639  0.16607615 0.33688453 0.         0.21901351 0.30466324\n",
      " 0.10295544 0.1936796  0.50351834 0.3724767  0.29708183 0.31160992\n",
      " 0.24022657 0.13084635 0.27899155 0.         0.11414108 0.17165199\n",
      " 0.18496117 0.15288004], the best loss is [0.15804014 0.16607615 0.13938925 0.         0.21901351 0.30466324\n",
      " 0.10295544 0.14656332 0.20510298 0.2423155  0.19065228 0.21318659\n",
      " 0.14573061 0.13084635 0.22469053 0.         0.11414108 0.14436111\n",
      " 0.14404178 0.15288004], and the best among neighbours is 3.\n",
      "the rankings of the classification target are: [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]. The top 10 ranking is: [[[4 9 7 5 0 8 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 8 0 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[9 4 7 5 8 0 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 8 0 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 8 0 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 1 6 3]]\n",
      "\n",
      " [[9 4 7 5 0 8 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 8 0 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]\n",
      "\n",
      " [[4 9 7 5 8 0 2 1 6 3]]\n",
      "\n",
      " [[4 9 7 5 0 8 2 6 1 3]]].\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "target_classifier = load_model('mnist_cnn_model.h5')\n",
    "attacker = PSOAttack(sess, target_classifier)\n",
    "\n",
    "benign_example = X_test[4:5]\n",
    "bening_y = y_test[4:5]\n",
    "\n",
    "pso_x = attacker.optimise(benign_example, bening_y) # non-targeted attack\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(pso_x.shape)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "axes[0].imshow(benign_example.reshape(28,28), cmap='gray')\n",
    "axes[1].imshow(pso_x[0].reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"2_2\"></a>\n",
    "\n",
    "## 2.2  Testing the attack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras classifier prediction on the benign example: 4\n",
      "Keras classifier prediction on the fgsm example: 9\n"
     ]
    }
   ],
   "source": [
    "# Input the examples into the classifier for classification\n",
    "print(f'Keras classifier prediction on the benign example: {np.argmax(target_classifier.predict(benign_example))}')\n",
    "print(f'Keras classifier prediction on the PSO example: {np.argmax(target_classifier.predict(pso_x[0]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3 Adversarial Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"3_1\"></a>\n",
    "\n",
    "## 3.1  Implementing adversarial training\n",
    "In this section, we will implement the adversarial training defence by incorporating FGSM gradients into the training. First, we will load tensorflow and keras. **MNIST** dataset is loaded for training and testing the defence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Activation, Multiply, BatchNormalization\n",
    "from keras.layers import LeakyReLU, PReLU\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, **we reuse the CNN model from last week**. We do not compile it since we are going to define its loss function and compile it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_CNN(In_Shape):\n",
    "    inputs = Input(shape=In_Shape, name='Normal_inputs')\n",
    "    x = Convolution2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', name='C1')(inputs)\n",
    "    x = Convolution2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same', name='C2')(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2), name='MP1')(x)\n",
    "    x = Flatten(name='Flatten')(x)\n",
    "    x = Dense(200, activation='relu', name='D1')(x)\n",
    "    x = Dense(10, name='logits')(x)\n",
    "    outputs = Activation('softmax', name='normal_output')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code snippet, we load the keras model into the main tensorflow graph. We then calculate the **FGSM** gradients and use it to build the loss function for the adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# FGSM hyper-parameter\n",
    "eps = 0.15\n",
    "\n",
    "# load model after the init op\n",
    "target_classifier = MNIST_CNN(X_train[0].shape)\n",
    "target_classifier.summary()\n",
    "\n",
    "# build graph using keras\n",
    "model_input = Input(X_train[0].shape)\n",
    "true_label = Input(y_train[0].shape)\n",
    "model_output = target_classifier(model_input)\n",
    "\n",
    "# adversarial loss function for FGSM\n",
    "adv_loss = K.categorical_crossentropy(model_output, true_label)\n",
    "# get FGSM example in real time\n",
    "dy_dx, = tf.gradients(adv_loss, model_input)\n",
    "x_adv = tf.stop_gradient(model_input + eps * tf.sign(dy_dx))\n",
    "x_adv = tf.clip_by_value(x_adv, 0, 1)\n",
    "# construct the loss function for the keras model\n",
    "adv_out = target_classifier(x_adv)\n",
    "adv_train_loss = K.categorical_crossentropy(model_output, true_label) + K.categorical_crossentropy(adv_out, true_label)\n",
    "\n",
    "# add the loss to the keras model\n",
    "model_to_be_trained = Model(inputs=[model_input, true_label], outputs=model_output)\n",
    "model_to_be_trained.add_loss(adv_train_loss)\n",
    "model_to_be_trained.compile(optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can call the **fit** method to train the model with defence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "model_to_be_trained.fit([X_train, y_train],\n",
    "                        epochs=num_epochs, \n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = \"3_2\"></a>\n",
    "\n",
    "## 3.2  Testing the defence\n",
    "We can now test the previously made adversarial examples on the model with defence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the examples into the classifier for classification\n",
    "print(f'Keras classifier prediction on the benign example: {np.argmax(target_classifier.predict(benign_example))}')\n",
    "print(f'Keras classifier prediction on the c&w example: {np.argmax(target_classifier.predict(cw_x))}')\n",
    "print(f'Keras classifier prediction on the PSO example: {np.argmax(target_classifier.predict(pso_x[0]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <div  style=\"text-align:center\">**THE END**</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
